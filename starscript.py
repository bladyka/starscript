import streamlit as st
from sentence_transformers import SentenceTransformer, util
import json
import os
import hashlib

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
model = SentenceTransformer('all-MiniLM-L6-v2')

BASE_CONCEPTS = {
    "water": "sel",
    "fire": "ver",
    "earth": "tur",
    "wind": "kai",
    "movement": "nol",
    "light": "phi",
    "darkness": "zor",
    "object": "kal",
    "life": "hal",
    "death": "mir",
    "tree": "lor",
    "metal": "zek",
    "sky": "tha",
    "sound": "rak",
    "energy": "ven",
    "emotion": "lii",
    "structure": "bal",
    "technology": "syn",
    "knowledge": "mek",
    "heat": "zor",
    "space": "qel",
    "body": "kor",
    "limb": "dra",
    "mother": "hala",
    "sun": "suur",
    "engine": "syn-nol-ven",
    "leg": "kor-dra",
    "human": "haleya",
    "love": "lii-hal",
    "boat": "kal-nol-sel-lor",
    "star": "suur"
}

# –ö—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤
DYNAMIC_DICT_PATH = "dynamic_dict.json"

def load_dynamic_dict():
    if os.path.exists(DYNAMIC_DICT_PATH):
        with open(DYNAMIC_DICT_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    else:
        return {}

def save_dynamic_dict(d):
    with open(DYNAMIC_DICT_PATH, "w", encoding="utf-8") as f:
        json.dump(d, f, ensure_ascii=False, indent=2)

dynamic_dict = load_dynamic_dict()

# –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –±–∞–∑—É –¥–ª—è –ø–æ–∏—Å–∫–∞
base_phrases = list(BASE_CONCEPTS.keys())
base_embeddings = model.encode(base_phrases, convert_to_tensor=True)

def semantic_translate(word):
    word_lower = word.lower()
    if word_lower in dynamic_dict:
        return dynamic_dict[word_lower], "Cached translation"

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
    query_emb = model.encode(word_lower, convert_to_tensor=True)
    hits = util.semantic_search(query_emb, base_embeddings, top_k=3)[0]

    # –°–æ–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥ –∏–∑ –ª—É—á—à–∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –µ—Å–ª–∏ –æ–Ω–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–ª–∏–∑–∫–∏
    threshold = 0.5
    parts = []
    explanation = []
    for hit in hits:
        if hit['score'] >= threshold:
            concept = base_phrases[hit['corpus_id']]
            parts.append(BASE_CONCEPTS[concept])
            explanation.append(f"{concept} ({hit['score']:.2f})")

    if not parts:
        # –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ—Ö–æ–∂–∏—Ö ‚Äî —Å–æ–∑–¥–∞—ë–º —Ö–µ—à –∏ —Å—Ç–∞–≤–∏–º –∑–Ω–∞–∫ –≤–æ–ø—Ä–æ—Å–∞
        hash_key = hashlib.md5(word_lower.encode()).hexdigest()[:4]
        alien_word = f"???-{hash_key}"
        dynamic_dict[word_lower] = alien_word
        save_dynamic_dict(dynamic_dict)
        return alien_word, "No similar concepts found"

    # –°–æ–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    alien_word = "-".join(sorted(set(parts)))
    dynamic_dict[word_lower] = alien_word
    save_dynamic_dict(dynamic_dict)
    return alien_word, " + ".join(explanation)

# Streamlit UI
st.title("üåå Zvezdniy Skript Ultimate Translator")
input_text = st.text_input("Enter any English word or phrase:")

if input_text:
    words = input_text.strip().split()
    translations = []
    explanations = []
    for w in words:
        tr, expl = semantic_translate(w)
        translations.append(tr)
        explanations.append(f"**{w}** ‚Üí {tr} ({expl})")

    st.subheader("Translation:")
    st.write(" ".join(translations))

    st.subheader("Explanation:")
    st.markdown("\n\n".join(explanations))
