import streamlit as st
import hashlib
import json
import os
import nltk
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')

DICT_PATH = "alien_dict.json"

if os.path.exists(DICT_PATH):
    with open(DICT_PATH, "r", encoding="utf-8") as f:
        alien_dict = json.load(f)
else:
    alien_dict = {}

lemmatizer = WordNetLemmatizer()

# –ù–∞–±–æ—Ä —Å–ª–æ–≥–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–æ–≤ (—É–¥–æ–±–æ—á–∏—Ç–∞–µ–º—ã–µ)
SYLLABLES = [
    "ka", "lo", "mi", "re", "zu", "na", "shi", "to", "va",
    "xi", "pa", "ru", "se", "di", "ne"
]

# –ë–∞–∑–∞ –∫–æ—Ä–Ω–µ–π –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö: –∫–æ—Ä–µ–Ω—å -> –∏–Ω–æ–ø–ª–∞–Ω–µ—Ç—Å–∫–∏–π –∫–æ—Ä–µ–Ω—å
ROOTS = {
    "water": "sel",
    "fire": "ver",
    "earth": "tur",
    "wind": "kai",
    "move": "nol",
    "light": "phi",
    "dark": "zor",
    "object": "kal",
    "life": "hal",
    "death": "mir",
    "tree": "lor",
    "metal": "zek",
    "sky": "tha",
    "sound": "rak",
    "energy": "ven",
    "emotion": "lii",
    "structure": "bal",
    "tech": "syn",
    "know": "mek",
    "heat": "zor",
    "space": "qel",
    "body": "kor",
    "limb": "dra",
    "mother": "hala",
    "sun": "suur",
    "engine": "syn-nol-ven",
    "leg": "kor-dra",
    "human": "haleya",
    "love": "lii-hal",
    "boat": "kal-nol-sel-lor",
    "star": "suur",
    "you": "va-lo",
    "your": "va-lo",
    # –î–æ–±–∞–≤–∏–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞
    "running": "nol",  # –æ—Ç move
    "run": "nol",
    "walk": "nol",
    "walked": "nol"
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ—Ä–Ω–µ–π –≤ —Å–ª–æ–≤–µ (–∏—â–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã–π –∫–æ—Ä–µ–Ω—å)
def find_roots(word):
    word = word.lower()
    roots_found = []
    # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∫–æ—Ä–Ω–∏, –Ω–∞—á–∏–Ω–∞—è —Å –¥–ª–∏–Ω–Ω—ã—Ö
    sorted_roots = sorted(ROOTS.keys(), key=lambda x: -len(x))
    pos = 0
    while pos < len(word):
        matched = False
        for root in sorted_roots:
            if word.startswith(root, pos):
                roots_found.append(ROOTS[root])
                pos += len(root)
                matched = True
                break
        if not matched:
            # –ï—Å–ª–∏ –∫–æ—Ä–µ–Ω—å –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –æ–¥–∏–Ω —Å–∏–º–≤–æ–ª –∫–∞–∫ –∑–∞–≥–ª—É—à–∫—É
            roots_found.append(word[pos])
            pos += 1
    return roots_found

def generate_alien_word(word):
    # –î–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏: –∏–∑ —Ö–µ—à–∞ –¥–µ–ª–∞–µ–º –Ω–∞–±–æ—Ä —Å–ª–æ–≥–æ–≤
    h = hashlib.md5(word.encode()).hexdigest()
    sylls = []
    for i in range(0, 6, 2):
        val = int(h[i:i+2], 16)
        sylls.append(SYLLABLES[val % len(SYLLABLES)])
    return "-".join(sylls)

def to_lemma(word):
    return lemmatizer.lemmatize(word.lower())

def translate_word(word):
    w = word.lower()
    if w in alien_dict:
        return alien_dict[w], "From cache"
    lemma = to_lemma(w)
    roots = find_roots(lemma)
    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Ç—å –æ–¥–∏–Ω –∫–æ—Ä–µ–Ω—å –∏–∑ —Å–ª–æ–≤–∞—Ä—è, —Å–æ–µ–¥–∏–Ω—è–µ–º –∏—Ö
    if any(r in ROOTS.values() for r in roots):
        alien_word = "-".join(roots)
    else:
        alien_word = generate_alien_word(lemma)
    alien_dict[w] = alien_word
    with open(DICT_PATH, "w", encoding="utf-8") as f:
        json.dump(alien_dict, f, ensure_ascii=False, indent=2)
    return alien_word, "Translated"

def translate_phrase(phrase):
    words = phrase.strip().split()
    translated_parts = [translate_word(w)[0] for w in words]
    return "-".join(translated_parts)

st.title("üöÄ Zvezdniy Skript Improved Translator")

text = st.text_input("Enter any English word or phrase:")

if text:
    translation = translate_phrase(text)
    st.subheader("Translation:")
    st.write(translation)
